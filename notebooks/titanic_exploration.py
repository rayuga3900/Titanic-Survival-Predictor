# -*- coding: utf-8 -*-
"""titanic_exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6UAGT6Vu4IZY3Js8of69xFrP8Vd8UFg
"""

import sklearn
print(sklearn.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Don’t fit (learn) on test data.
But do apply the preprocessing transform to every dataset (train, test, unseen).
"""

import pandas as pd
df = pd.read_csv("titanic.csv")
df.head()

sns.pairplot(df)
# sns.heatmap(df)

df.describe()

df.info()

"""Checking for the duplicates"""

df = df.drop(df[df.duplicated()].index)
df.duplicated().sum()

"""Adding new features"""

df['FamilySize'] = df['sibsp'] + df['parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
# Extracting title from name
df['Title'] = df['name'].str.extract(r' ([A-Za-z]+)\.', expand=False)

# Grouping  rare titles
df['Title'] = df['Title'].replace(
    ['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],
    'Rare'
)
df['Title'] = df['Title'].replace({'Mlle':'Miss','Ms':'Miss','Mme':'Mrs'})

"""Splitting into feature and target"""

x = df.drop('survived',axis=1)
y = df['survived']

x.head()

"""Checking for class imbalance
| **Minority:Majority Ratio** | **Interpretation** | **Suggested Action**               |
| --------------------------- | ------------------ | ---------------------------------- |
| 1 – 1.5                     | Balanced           | No action needed                   |
| 1.5 – 3                     | Slight imbalance   | Consider `class_weight`            |
| 3 – 5                       | Moderate imbalance | Use `class_weight` or oversampling |
| Greater than 5              | Severe imbalance   | SMOTE / undersampling required     |

"""

y.value_counts()

counts = y.value_counts()
ratio = counts.max() / counts.min()
print(f"Ratio: {ratio:.2f}x")  # 1.72x
print("Imbalanced" if ratio > 1.5 else "Balanced")
counts / len(y)  # Proportions: ~0.63, 0.37

"""Splitting into train , test data"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Some categories are stored as number type so changing their datatype"""

x_train['pclass'] =x_train['pclass'].astype('category')
x_train['sibsp'] = x_train['sibsp'].astype('category')
x_train['parch'] = x_train['parch'].astype('category')
x_test['pclass'] = x_test['pclass'].astype('category')
x_test['sibsp'] = x_test['sibsp'].astype('category')
x_test['parch'] = x_test['parch'].astype('category')

print(x_train.info())
print(x_test.info())

"""The data has many missing values and you see it is not helping you in prediction or it occurs after prediction has happens, drop it"""

dropped_columns = ['body','cabin','home.dest','boat','passenger_id','name','ticket']
#dropping the columns which are not useful for prediction some of them are
#post-event info like after surviving which boat they took ->boat
#how many bodies were found - body
x_train = x_train.drop(dropped_columns , axis=1)
x_test = x_test.drop(dropped_columns, axis=1)
print(x_train.info())
print(x_test.info())

x_num = x_train.select_dtypes(include=['int64','float64'])
x_cat = x_train.select_dtypes(include=['object','category'])
x_num_test = x_test.select_dtypes(include=['int64', 'float64'])
x_cat_test = x_test.select_dtypes(include=['object', 'category'])
print(x_num)
print(x_cat)
print(x_num_test)
print(x_cat_test)

"""Checking if null value exist"""

x_num.isnull().sum()

"""Using median imputer for age and fare for missing values"""

x_num['age'] = x_num['age'].fillna(x_num['age'].median())
x_num['fare'] = x_num['fare'].fillna(x_num['fare'].median())
x_num_test['age'] = x_num_test['age'].fillna(x_num['age'].median())
x_num_test['fare'] = x_num_test['fare'].fillna(x_num['fare'].median())


print(x_num.isnull().sum())
print(x_num_test.isnull().sum())

x_num

"""Checking for the outliers

Reason we are not clipping the outlier is that outlier is not error here they are correct values like In fare paying high for 1st class , in age some maybe old some may be young
"""

# outliers_dict = {}

# for col in x_num.columns:
#     q1 = x_num[col].quantile(0.25)
#     q3 = x_num[col].quantile(0.75)
#     iqr = q3 - q1
#     lower = q1 - 1.5 * iqr
#     upper = q3 + 1.5 * iqr
#     mask = (x_num[col] < lower) | (x_num[col] > upper)
#     outliers_dict[col] = x_num[col][mask]

# # Print outliers per column
# for col, outliers in outliers_dict.items():
#     print(f"Outliers in {col}:\n{outliers}\n")

"""finding and clipping the outliers"""

# def clip_outliers(series):
#     q1, q3 = series.quantile([0.25, 0.75])
#     iqr = q3 - q1
#     lower = q1 - 1.5 * iqr
#     upper = q3 + 1.5 * iqr
#     return series.clip(lower, upper)

# x_num['age'] = clip_outliers(x_num['age'])
# x_num['fare'] = clip_outliers(x_num['fare'])

# x_num_test['age'] = clip_outliers(x_num_test['age'])
# x_num_test['fare'] = clip_outliers(x_num_test['fare'])

"""Checking the skewness

| **Skewness Value** | **Interpretation**           |
| ------------------ | ---------------------------- |
| -0.5 to 0.5        | Approximately symmetric      |
| 0.5 to 1           | Moderately positively skewed |
| Greater than 1     | Highly positively skewed     |
| -1 to -0.5         | Moderately negatively skewed |
| Less than -1       | Highly negatively skewed     |


"""

x_num.skew()#fare is skewed

"""Handling the skewness via transformation"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PowerTransformer, StandardScaler

skewed_cols = ['fare']   # only the skewed ones
non_skewed_cols = ['age']  # or whatever columns are not skewed

ct = ColumnTransformer(
    transformers=[
        ('yeojohnson', PowerTransformer(method='yeo-johnson'), skewed_cols)
    ],
    remainder='passthrough'   # keep all other columns as they are
)
num_scaler = StandardScaler()

x_num_final = ct.fit_transform(x_num)
x_num_final = pd.DataFrame(
    num_scaler.fit_transform(x_num_final),
    columns=skewed_cols + [c for c in x_num.columns if c not in skewed_cols],
    index=x_num.index
)

x_num_test_final = ct.transform(x_num_test)
x_num_test_final = pd.DataFrame(
    num_scaler.transform(x_num_test_final),
    columns=skewed_cols + [c for c in x_num.columns if c not in skewed_cols],
    index=x_num_test.index
)

print(x_num_final)
print(x_num_test_final)

"""Checking missing values in Categorical features"""

x_cat.isnull().sum()

"""Encoding the categorical features"""

# from sklearn.preprocessing import OneHotEncoder

# x_cat = OneHotEncoder().fit_transform(x_cat)
# x_cat

# x_cat.toarray()

"""Checking the correlation"""

x_num_final.corr()

"""checking the variance"""

x_num_final.var()

print(x_cat.isnull().sum())

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)
# drop='first' avoids multicollinearity[highly correlated features]: for a binary category like 'sex',
# if 'male' = 0, we automatically know it's 'female' = 1, so we only need one column.
x_cat_encoded = ohe.fit_transform(x_cat)
x_cat_encoded = pd.DataFrame(x_cat_encoded, columns=ohe.get_feature_names_out(x_cat.columns), index=x_cat.index)

x_cat_test_encoded = ohe.transform(x_cat_test)
x_cat_test_encoded = pd.DataFrame(x_cat_test_encoded, columns=ohe.get_feature_names_out(x_cat.columns), index=x_cat_test.index)

x_train_final = pd.concat([x_num_final, x_cat_encoded], axis=1)
x_test_final = pd.concat([x_num_test_final, x_cat_test_encoded], axis=1)

"""Training the model"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    roc_auc_score
)
from sklearn.tree import DecisionTreeClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(
        max_depth=5,
        min_samples_leaf=10,
        random_state=42
    )
}


results = {}
# custom threshold by default 0.5
# threshold = 0.7
# preds_custom = (proba[:,1] >= threshold).astype(int)

for name, model in models.items():
    model.fit(x_train_final, y_train)
    preds = model.predict(x_test_final)
    proba = model.predict_proba(x_test_final)

    results[name] = {
        "Accuracy": accuracy_score(y_test, preds),
        "Precision (macro)": precision_score(y_test, preds, average='macro'),
        "Recall (macro)": recall_score(y_test, preds, average='macro'),
        "F1 (macro)": f1_score(y_test, preds, average='macro'),
        "Confusion Matrix": confusion_matrix(y_test, preds),
        "AUC": roc_auc_score(y_test, proba[:,1])
    }



print(results)

"""Groupwise average survival probability"""

x_test_copy = x_test.copy()
# Use original x_test for group-level analysis, since preprocessed features aren't human-readable.
# Probabilities from the model can be safely mapped back to x_test.

x_test_copy['Predicted_Survival_Prob'] = proba[:,1]

print(x_test_copy.groupby('sex')['Predicted_Survival_Prob'].mean())
print(x_test_copy.groupby('pclass')['Predicted_Survival_Prob'].mean())
print(x_test_copy.groupby('Title')['Predicted_Survival_Prob'].mean())
print(x_test_copy.groupby('FamilySize')['Predicted_Survival_Prob'].mean())

# from sklearn.metrics import (
#     accuracy_score,
#     precision_score,
#     recall_score,
#     f1_score,
#     confusion_matrix,
#     roc_auc_score
# )

# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.compose import ColumnTransformer
# from sklearn.pipeline import Pipeline
# from sklearn.preprocessing import StandardScaler, OneHotEncoder
# from sklearn.impute import SimpleImputer
# from sklearn.model_selection import GridSearchCV
# # Numeric pipeline: fill missing with mean, then scale
# numeric_transformer = Pipeline([
#     ('imputer', SimpleImputer(strategy='mean')),
#     ('scaler', StandardScaler())
# ])

# # Categorical pipeline: fill missing with most frequent, then one-hot encode
# categorical_transformer = Pipeline([
#     ('imputer', SimpleImputer(strategy='most_frequent')),
#     ('onehot', OneHotEncoder(handle_unknown='ignore'))
# ])

# # ColumnTransformer combines both
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('num', numeric_transformer, x_num_final.columns),
#         ('cat', categorical_transformer, x_cat.columns)
#     ])

# models = {
#     "Logistic Regression": Pipeline([
#         ('preprocess', preprocessor),
#         ('logreg', LogisticRegression(max_iter=500))
#     ]),

#     "SVM": Pipeline([
#         ('preprocess', preprocessor),
#         ('svc', SVC(probability=True))
#     ]),
#     "KNN": Pipeline([
#         ('preprocess', preprocessor),
#         ('knn', KNeighborsClassifier(n_neighbors=5))
#     ])
# }
# results = {}

# for name, model in models.items():
#     model.fit(x_train, y_train)
#     preds = model.predict(x_test)
#     proba = model.predict_proba(x_test)

#     results[name] = {
#         "Accuracy": accuracy_score(y_test, preds),
#         "Precision (macro)": precision_score(y_test, preds, average='macro'),
#         "Recall (macro)": recall_score(y_test, preds, average='macro'),
#         "F1 (macro)": f1_score(y_test, preds, average='macro'),
#         "Confusion Matrix": confusion_matrix(y_test, preds)
#     }


#     n_classes = len(np.unique(y_train))
#     if n_classes == 2:
#         # Binary: use probability of positive class (index 1)
#         results[name]["AUC"] = roc_auc_score(y_test, proba[:, 1])
#     else:
#         # Multi-class: use full probability matrix
#         results[name]["AUC (ovr)"] = roc_auc_score(y_test, proba, multi_class="ovr")

# print(results)

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [1,2,3,4,5,6,7,8,9,10],
    'min_samples_leaf': [10,15,20,25],
    'min_samples_split': [10,15,20,25,30]
}


grid = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    scoring='roc_auc',
    cv=5,
    n_jobs=-1
)

grid.fit(x_train_final, y_train)

best_decision_tree= grid.best_estimator_

print("Best Params:", grid.best_params_)
print("Best CV AUC:", grid.best_score_)

"""roc curve to see the TPR vs FPR"""

from sklearn.metrics import roc_curve, auc

y_proba = best_decision_tree.predict_proba(x_test_final)[:,1]

fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Manual Titanic")
plt.legend()
plt.show()

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_estimator(best_decision_tree, x_test_final, y_test)
plt.title("ROC Curve – Titanic")
plt.show()

"""PR curve"""

from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, _ = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)

plt.figure(figsize=(6,5))
plt.plot(recall, precision, label=f"AP = {ap:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve – Manual Titanic")
plt.legend()
plt.show()

from sklearn.metrics import PrecisionRecallDisplay

PrecisionRecallDisplay.from_estimator(best_decision_tree, x_test_final, y_test)
plt.title("Precision-Recall Curve – Titanic")
plt.show()

"""Learning Curve

X-axis → Size of the training set

Y-axis → Performance metric (train score and validation score)

A learning curve trains the model multiple times, each time using a larger portion of the training data. For each size, it calculates the performance metric (like accuracy, AUC, or loss) on both:

The training set used for that run

The validation/test set

Then it plots the metric vs. training size.
"""

from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    best_decision_tree,
    x_train_final,
    y_train,
    cv=5,
    scoring='roc_auc',
    train_sizes=np.linspace(0.1, 1.0, 5),
    n_jobs=-1
)

plt.figure(figsize=(6,5))
plt.plot(train_sizes, train_scores.mean(axis=1), label='Train AUC')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation AUC')
plt.xlabel("Training Size")
plt.ylabel("AUC")
plt.title("Learning Curve – Manual Titanic")
plt.legend()
plt.show()

from sklearn.model_selection import learning_curve
from sklearn.model_selection import LearningCurveDisplay

train_sizes, train_scores, val_scores = learning_curve(
    best_decision_tree, x_train_final, y_train, cv=5, scoring='roc_auc', n_jobs=-1
)

display = LearningCurveDisplay(
    train_sizes=train_sizes,
    train_scores=train_scores,
    test_scores=val_scores
).plot()
# display.plot()
plt.title("Learning Curve – Titanic")
plt.show()

"""To see how many correct and incorrect predictions were made by the model on train and test dataset"""

from sklearn.metrics import ConfusionMatrixDisplay

# Train
ConfusionMatrixDisplay.from_estimator(
    best_decision_tree, x_train_final, y_train
)
plt.title("Train Confusion Matrix")
plt.show()

# Test
ConfusionMatrixDisplay.from_estimator(
    best_decision_tree, x_test_final, y_test
)
plt.title("Test Confusion Matrix")
plt.show()

y_pred = best_decision_tree.predict_proba(x_test_final)[:,1]
residuals = y_test - y_pred

plt.scatter(y_pred, residuals)
plt.axhline(0, color='red')
plt.xlabel("Predicted value")
plt.ylabel("Residual")
plt.show()

